{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loss-and-Metrics\" data-toc-modified-id=\"Loss-and-Metrics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loss and Metrics</a></span></li><li><span><a href=\"#Camargo\" data-toc-modified-id=\"Camargo-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Camargo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spezialized\" data-toc-modified-id=\"Spezialized-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Spezialized</a></span></li><li><span><a href=\"#Concat\" data-toc-modified-id=\"Concat-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Concat</a></span></li><li><span><a href=\"#Full_Concat\" data-toc-modified-id=\"Full_Concat-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Full_Concat</a></span></li><li><span><a href=\"#PPMS\" data-toc-modified-id=\"PPMS-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>PPMS</a></span></li></ul></li><li><span><a href=\"#Evermann\" data-toc-modified-id=\"Evermann-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evermann</a></span><ul class=\"toc-item\"><li><span><a href=\"#PPM\" data-toc-modified-id=\"PPM-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>PPM</a></span></li></ul></li><li><span><a href=\"#Tax\" data-toc-modified-id=\"Tax-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Tax</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spezialized\" data-toc-modified-id=\"Spezialized-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Spezialized</a></span></li><li><span><a href=\"#Shared\" data-toc-modified-id=\"Shared-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Shared</a></span></li><li><span><a href=\"#Mixed\" data-toc-modified-id=\"Mixed-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Mixed</a></span></li><li><span><a href=\"#PPM\" data-toc-modified-id=\"PPM-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>PPM</a></span></li></ul></li><li><span><a href=\"#Mida\" data-toc-modified-id=\"Mida-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Mida</a></span><ul class=\"toc-item\"><li><span><a href=\"#PPM\" data-toc-modified-id=\"PPM-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>PPM</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline implementations\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces a few loss and metric functions. Afterwards, eight models based on four papers are re-implemented in `pytorch`. For each model, a `PPModel` is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext memory_profiler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from mppn.imports import *\n",
    "from mppn.preprocessing import *\n",
    "from mppn.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a flag, that controls if the training process is executed\n",
    "_RUN_TRAINING=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines some metrics and loss functions. Apart from that, we use the standart loss functions and metrics from fastai and pytorch, namely accuracy, mae, and cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maeDurDaysNormalize(p,yb,mean=0,std=0,unit=60*60*24):\n",
    "    \"\"\"\n",
    "    Decodes time and converts from seconds to days\n",
    "    Returns mae\n",
    "    \"\"\"\n",
    "    p=p*std+mean\n",
    "    yb=yb*std+mean\n",
    "    return mae(p,yb)/(unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maeDurDaysMinMax(p,yb,minn=0,maxx=0,unit=60*60*24):\n",
    "    \"\"\"\n",
    "    Decodes time and converts from seconds to days\n",
    "    Returns mae\n",
    "    \"\"\"\n",
    "\n",
    "    p=p*(maxx-minn) + minn\n",
    "    yb=yb*(maxx-minn) + minn\n",
    "    return mae(p,yb)/(unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _accuracy_idx(a,b,i): return accuracy(listify(a)[i],listify(b)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgMetric(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func):  self.func = func\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(self.func(learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.func.__name__ if hasattr(self.func, '__name__') else self.func.func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_metrics(o,date_col='timestamp_Relative_elapsed'):\n",
    "    'A utility function that automatically selects the correct metric functions based on the PPObj o'\n",
    "    number_cats=len(o.ycat_names)\n",
    "\n",
    "    accuracies=[]\n",
    "    for i in range(number_cats):\n",
    "        accuracy_func=partial(_accuracy_idx,i=i)\n",
    "        accuracy_func.__name__= f\"acc_{o.ycat_names[i]}\"\n",
    "        accuracy_func=AvgMetric(accuracy_func)\n",
    "        accuracies.append(accuracy_func)\n",
    "    mae_days=None\n",
    "    if len(o.ycont_names)>0:\n",
    "        if 'minmax' in o.ycont_names[0]: # Here we expect only one timestamp\n",
    "            minn,maxx = (o.procs.min_max.mins[date_col],\n",
    "                         o.procs.min_max.maxs[date_col])\n",
    "            mae_days=lambda p,y: maeDurDaysMinMax(listify(p)[-1],listify(y)[-1],minn=minn,maxx=maxx)\n",
    "        else:\n",
    "            mean,std=(o.procs.normalize.means[date_col],\n",
    "                      o.procs.normalize.stds[date_col])\n",
    "            mae_days=lambda p,y: maeDurDaysNormalize(listify(p)[-1],listify(y)[-1],mean=mean,std=std)\n",
    "        mae_days.__name__='mae_days'\n",
    "    return L(accuracies)+mae_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def multi_loss_sum(o,p,y):\n",
    "    '''Multi Loss function that sums up multiple loss functions. \n",
    "    The selection of the loss function is based on the PPObj o'''\n",
    "    p,y=listify(p),listify(y)\n",
    "    len_cat,len_cont=len(o.ycat_names),len(o.ycont_names)\n",
    "    cross_entropies=[F.cross_entropy(p[i],y[i]) for i in range(len_cat)]\n",
    "    maes=[mae(p[i],y[i]) for i in range(len_cat,len_cat+len_cont)]\n",
    "    return torch.sum(torch.stack(list(L(cross_entropies)+L(maes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camargo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: activity, resource, duration  \n",
    "**Output**: activity, resource, duration  \n",
    "**Loss**: sum(cross_entropy(activity),cross_entropy(resource),mae(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log(EventLogs.Helpdesk)\n",
    "o=PPObj(log,[Categorify,Datetify,Normalize()],date_names=['timestamp'],cat_names=['activity','resource'],\n",
    "    y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log))\n",
    "o\n",
    "dls=o.get_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcat,xcont,y=dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2, 64]), torch.Size([64, 64]), 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcat.shape,xcont.shape,len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spezialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Camargo_specialized(torch.nn.Module) :\n",
    "    def __init__(self, o) :\n",
    "        super().__init__()\n",
    "        hidden=25\n",
    "        vocab_act=len(o.procs.categorify['activity'])\n",
    "        vocab_res=len(o.procs.categorify['resource'])\n",
    "        emb_dim_act = int(sqrt(vocab_act))+1\n",
    "        emb_dim_res = int(sqrt(vocab_res))+1\n",
    "\n",
    "        self.emb_act = nn.Embedding(vocab_act,emb_dim_act)\n",
    "        self.emb_res = nn.Embedding(vocab_res,emb_dim_res)\n",
    "\n",
    "        self.lstm_act = nn.LSTM(emb_dim_act, hidden, batch_first=True, num_layers=2)\n",
    "        self.lstm_res = nn.LSTM(emb_dim_res, hidden, batch_first=True, num_layers=2)\n",
    "        self.lstm_tim = nn.LSTM(1, hidden, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden, vocab_act)\n",
    "        self.linear_res = nn.Linear(hidden, vocab_res)\n",
    "        self.linear_tim = nn.Linear(hidden, 1)\n",
    "    def forward(self, xcat,xcont):\n",
    "        x_act,x_res,x_tim=xcat[:,0],xcat[:,1],xcont[:,:,None]\n",
    "        x_act = self.emb_act(x_act)\n",
    "        x_act,_ = self.lstm_act(x_act)\n",
    "        x_act = x_act[:,-1]\n",
    "        x_act = self.linear_act(x_act)\n",
    "        x_act = F.softmax(x_act,dim=1)\n",
    "\n",
    "        x_res = self.emb_res(x_res)\n",
    "        x_res,_ = self.lstm_res(x_res)\n",
    "        x_res = x_res[:,-1]\n",
    "        x_res = self.linear_res(x_res)\n",
    "        x_res = F.softmax(x_res,dim=1)\n",
    "\n",
    "        x_tim,_ = self.lstm_tim(x_tim)\n",
    "        x_tim = x_tim[:,-1]\n",
    "        x_tim = self.linear_tim(x_tim)\n",
    "        return x_act,x_res,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Camargo_specialized(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 15]), torch.Size([64, 23]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(i.shape for i in p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    log=import_log(EventLogs.Helpdesk)\n",
    "    o=PPObj(log,[Categorify,Datetify,Normalize],date_names=['timestamp'],cat_names=['activity','resource'],\n",
    "        y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log),)\n",
    "    dls=o.get_dls()\n",
    "    m=Camargo_specialized(o)\n",
    "    loss=0\n",
    "    metrics=get_metrics(o)\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,print_output=True,output_index=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Camargo_concat(torch.nn.Module) :\n",
    "    def __init__(self, o ) :\n",
    "        super().__init__()\n",
    "        hidden=25\n",
    "        vocab_act=len(o.procs.categorify['activity'])\n",
    "        vocab_res=len(o.procs.categorify['resource'])\n",
    "        emb_dim_act = int(sqrt(vocab_act))+1\n",
    "        emb_dim_res = int(sqrt(vocab_res))+1\n",
    "\n",
    "        self.emb_act = nn.Embedding(vocab_act,emb_dim_act)\n",
    "        self.emb_res = nn.Embedding(vocab_res,emb_dim_res)\n",
    "\n",
    "        self.lstm_concat= nn.LSTM(emb_dim_act+emb_dim_res, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_act = nn.LSTM(hidden, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_res = nn.LSTM(hidden, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_tim = nn.LSTM(1, hidden, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden, vocab_act)\n",
    "        self.linear_res = nn.Linear(hidden, vocab_res)\n",
    "        self.linear_tim = nn.Linear(hidden, 1)\n",
    "    def forward(self, xcat,xcont):\n",
    "        x_act,x_res,x_tim=xcat[:,0],xcat[:,1],xcont[:,:,None]\n",
    "        x_act = self.emb_act(x_act)\n",
    "\n",
    "        x_res = self.emb_res(x_res)\n",
    "        x_concat=torch.cat((x_act, x_res), 2)\n",
    "        x_concat,_=self.lstm_concat(x_concat)\n",
    "\n",
    "        x_act,_ = self.lstm_act(x_concat)\n",
    "        x_act = x_act[:,-1]\n",
    "        x_act = self.linear_act(x_act)\n",
    "        x_act = F.softmax(x_act,dim=1)\n",
    "\n",
    "        x_res,_ = self.lstm_res(x_concat)\n",
    "        x_res = x_res[:,-1]\n",
    "        x_res = self.linear_res(x_res)\n",
    "        x_res = F.softmax(x_res,dim=1)\n",
    "\n",
    "        x_tim,_ = self.lstm_tim(x_tim)\n",
    "        x_tim = x_tim[:,-1]\n",
    "        x_tim = self.linear_tim(x_tim)\n",
    "        return x_act,x_res,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Camargo_concat(o)\n",
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "\n",
    "    log=import_log(EventLogs.Helpdesk)\n",
    "    o=PPObj(log,[Categorify,Datetify,Normalize()],date_names=['timestamp'],cat_names=['activity','resource'],\n",
    "        y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log),)\n",
    "    dls=o.get_dls()\n",
    "    m=Camargo_concat(o)\n",
    "    loss=0\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    train_validate(dls,m,loss=loss,metrics=get_metrics(o),epoch=1,print_output=True,output_index=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full_Concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Camargo_fullconcat(torch.nn.Module) :\n",
    "\n",
    "\n",
    "    def __init__(self, o  ) :\n",
    "        super().__init__()\n",
    "\n",
    "        hidden=25\n",
    "        vocab_act=len(o.procs.categorify['activity'])\n",
    "        vocab_res=len(o.procs.categorify['resource'])\n",
    "        emb_dim_act = int(sqrt(vocab_act))+1\n",
    "        emb_dim_res = int(sqrt(vocab_res))+1\n",
    "\n",
    "        self.emb_act = nn.Embedding(vocab_act,emb_dim_act)\n",
    "        self.emb_res = nn.Embedding(vocab_res,emb_dim_res)\n",
    "\n",
    "        self.lstm_concat= nn.LSTM(emb_dim_act+emb_dim_res+1, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_act = nn.LSTM(hidden, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_res = nn.LSTM(hidden, hidden, batch_first=True, num_layers=1)\n",
    "        self.lstm_tim = nn.LSTM(hidden, hidden, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden, vocab_act)\n",
    "        self.linear_res = nn.Linear(hidden, vocab_res)\n",
    "        self.linear_tim = nn.Linear(hidden, 1)\n",
    "    def forward(self, xcat,xcont):\n",
    "        x_act,x_res,x_tim=xcat[:,0],xcat[:,1],xcont[:,:,None]\n",
    "        x_act = self.emb_act(x_act)\n",
    "\n",
    "        x_res = self.emb_res(x_res)\n",
    "        x_concat=torch.cat((x_act, x_res,x_tim), 2)\n",
    "        x_concat,_=self.lstm_concat(x_concat)\n",
    "\n",
    "        x_act,_ = self.lstm_act(x_concat)\n",
    "        x_act = x_act[:,-1]\n",
    "        x_act = self.linear_act(x_act)\n",
    "        x_act = F.softmax(x_act,dim=1)\n",
    "\n",
    "        x_res,_ = self.lstm_res(x_concat)\n",
    "        x_res = x_res[:,-1]\n",
    "        x_res = self.linear_res(x_res)\n",
    "        x_res = F.softmax(x_res,dim=1)\n",
    "\n",
    "        x_tim,_ = self.lstm_tim(x_concat)\n",
    "        x_tim = x_tim[:,-1]\n",
    "        x_tim = self.linear_tim(x_tim)\n",
    "        return x_act,x_res,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Camargo_fullconcat(o)\n",
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if  _RUN_TRAINING:\n",
    "    log=import_log(EventLogs.Helpdesk)\n",
    "    o=PPObj(log,[Categorify,Datetify,Normalize],date_names=['timestamp'],cat_names=['activity','resource'],\n",
    "        y_names=['activity','resource','timestamp_Relative_elapsed'],splits=split_traces(log),)\n",
    "    dls=o.get_dls()\n",
    "    m=Camargo_fullconcat(o)\n",
    "    loss=0\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    train_validate(dls,m,loss=loss,metrics=get_metrics(o),epoch=1,print_output=True,output_index=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PPM_Camargo_Spezialized(PPModel):\n",
    "\n",
    "    model = Camargo_specialized\n",
    "    date_names=['timestamp']\n",
    "    cat_names=['activity','resource']\n",
    "    y_names=['activity','resource','timestamp_Relative_elapsed']\n",
    "    cont_names=None\n",
    "    procs=[Categorify,Datetify,Normalize,FillMissing]\n",
    "\n",
    "    def setup(self):\n",
    "        o=PPObj(self.log,self.procs,cat_names=self.cat_names,date_names=self.date_names,y_names=self.y_names,\n",
    "                cont_names=self.cont_names,splits=self.splits)\n",
    "\n",
    "        loss=partial(multi_loss_sum,o)\n",
    "\n",
    "        # Next event prediction training\n",
    "        print('Next event prediction training')\n",
    "        dls=o.get_dls(bs=self.bs)\n",
    "        m=self.model(o)\n",
    "        self.nsp,self.nrp,self.dtnp=self._train_validate(dls,m,loss=loss,metrics=get_metrics(o),\n",
    "                                                   output_index=[1,2,3])\n",
    "\n",
    "\n",
    "        # Last event prediction training\n",
    "        print('Last event prediction training')\n",
    "        dls=o.get_dls(outcome=True,bs=self.bs)\n",
    "        m=self.model(o)\n",
    "        self.op,self.lrp,self.dtlp=self._train_validate(dls,m,loss=loss,metrics=get_metrics(o),\n",
    "                                                 output_index=[1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "    def next_step_prediction(self): return self.nsp\n",
    "\n",
    "    def next_resource_prediction(self):return self.nrp\n",
    "\n",
    "    def last_resource_prediction(self): return self.lrp\n",
    "    def outcome_prediction(self): return self.op\n",
    "    def duration_to_next_event_prediction(self): return self.dtnp\n",
    "    def duration_to_end_prediction(self): return self.dtlp\n",
    "    def activity_suffix_prediction(self): pass\n",
    "    def resource_suffix_prediction(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PPM_Camargo_concat(PPM_Camargo_Spezialized):\n",
    "    model = Camargo_concat\n",
    "\n",
    "class PPM_Camargo_fullconcat(PPM_Camargo_Spezialized):\n",
    "    model = Camargo_fullconcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    path=EventLogs.Helpdesk\n",
    "    log=import_log(path)\n",
    "    ppm=PPM_Camargo_fullconcat(log,get_ds_name(path),print_output=True,epoch=1,bs=512,\n",
    "                               splits=split_traces(log))\n",
    "    ppm.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evermann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**:  activity or resource  \n",
    "**Output**: activity or resource  \n",
    "**Loss**: cross_entropy(activity) or cross_entropy(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Evermann(torch.nn.Module) :\n",
    "    def __init__(self, o) :\n",
    "        super().__init__()\n",
    "        vocab_size=len(o.procs.categorify[o.y_names[0]])\n",
    "        hidden_dim=125\n",
    "        emb_dim = 5\n",
    "\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x.squeeze())\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x[:,-1])\n",
    "        return F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "col='activity'\n",
    "o=PPObj(log,procs=[Categorify],cat_names=col,y_names=col,splits=split_traces(log))\n",
    "dls=o.get_dls()\n",
    "xb,yb=dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64]), torch.Size([64]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Evermann(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.7060, grad_fn=<NllLossBackward>), TensorBase(0.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb=m(xb)\n",
    "F.cross_entropy(pb,yb),accuracy(pb,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7060, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial(multi_loss_sum,o)(pb,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=partial(multi_loss_sum,o)\n",
    "metrics=get_metrics(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PPM_Evermann(PPM_RNNwEmbedding): \n",
    "    model = Evermann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    runner([EventLogs.Helpdesk,EventLogs.Mobis],[PPM_Evermann,PPM_RNNwEmbedding],\n",
    "           bs=1024,epoch=1,print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**:  activity or duration  \n",
    "**Output**: activity or duration  \n",
    "**Loss**: cross_entropy(activity) or mae(duration) in days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spezialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Tax_et_al_spezialized(torch.nn.Module) :\n",
    "    def __init__(self,o) :\n",
    "        super().__init__()\n",
    "        vocab_size=len(o.procs.categorify[o.y_names[0]])\n",
    "        hidden_dim=125\n",
    "        self.lstm_act = nn.LSTM(vocab_size, hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.lstm_tim = nn.LSTM(3, hidden_dim, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.linear_tim = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, xcat,xcont):\n",
    "        x_act,x_tim = xcat.permute(0,2,1),xcont.squeeze().permute(0,2,1)\n",
    "        x_act, _ = self.lstm_act(x_act.float())\n",
    "        x_act=self.linear_act(x_act[:,-1])\n",
    "        x_act=F.softmax(x_act,dim=1)\n",
    "        x_tim, _ = self.lstm_tim(x_tim)\n",
    "        x_tim=self.linear_tim(x_tim[:,-1])\n",
    "        return x_act,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log(EventLogs.BPIC_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n",
    "o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n",
    "        date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n",
    "dls=o.get_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcat,xcont,yb=(dls.one_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) ['timestamp_secSinceSunNoon','timestamp_secSinceNoon','timestamp_Relative_elapsed']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Tax_et_al_spezialized(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(4.5369, grad_fn=<AliasBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=partial(multi_loss_sum,o)\n",
    "loss(p,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=get_metrics(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _RUN_TRAINING:\n",
    "    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n",
    "    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n",
    "            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n",
    "    dls=o.get_dls()\n",
    "    m=Tax_et_al_spezialized(o)\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    metrics=get_metrics(o)\n",
    "    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,output_index=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Tax_et_al_shared(torch.nn.Module) :\n",
    "    def __init__(self,o) :\n",
    "        super().__init__()\n",
    "        vocab_size=len(o.procs.categorify[o.y_names[0]])\n",
    "        hidden_dim=125\n",
    "        self.lstm = nn.LSTM(vocab_size+3, hidden_dim, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.linear_tim = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self,xcat,xcont):\n",
    "        x_act,x_tim = xcat.permute(0,2,1),xcont.squeeze().permute(0,2,1)\n",
    "\n",
    "        x_concat=torch.cat((x_act.float(), x_tim), 2)\n",
    "        x_concat, _ = self.lstm(x_concat)\n",
    "\n",
    "        x_act=self.linear_act(x_concat[:,-1])\n",
    "        x_act=F.softmax(x_act,dim=1)\n",
    "\n",
    "        x_tim=self.linear_tim(x_concat[:,-1])\n",
    "        return x_act,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Tax_et_al_shared(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(4.5371, grad_fn=<AliasBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(p,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _RUN_TRAINING:\n",
    "    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n",
    "    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=split_traces(log),\n",
    "            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n",
    "    m=Tax_et_al_shared(o)\n",
    "    dls=o.get_dls()\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    metrics=get_metrics(o)\n",
    "    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,output_index=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Tax_et_al_mixed(torch.nn.Module) :\n",
    "    def __init__(self,o,numlayers_shared=3,numlayers_single=3) :\n",
    "        super().__init__()\n",
    "        vocab_size=len(o.procs.categorify[o.y_names[0]])\n",
    "        hidden_dim=125\n",
    "\n",
    "        self.lstm_act = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=numlayers_single)\n",
    "        self.lstm_tim = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=numlayers_single)\n",
    "        self.lstm = nn.LSTM(vocab_size+3, hidden_dim, batch_first=True, num_layers=numlayers_shared)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.linear_tim = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self,xcat,xcont):\n",
    "        x_act,x_tim = xcat.permute(0,2,1),xcont.squeeze().permute(0,2,1)\n",
    "\n",
    "\n",
    "        x_concat=torch.cat((x_act.float(), x_tim), 2)\n",
    "        x_concat, _ = self.lstm(x_concat)\n",
    "\n",
    "        x_act, _ = self.lstm_act(x_concat)\n",
    "        x_act=self.linear_act(x_act[:,-1])\n",
    "        x_act=F.softmax(x_act,dim=1)\n",
    "\n",
    "        x_tim, _ = self.lstm_tim(x_concat)\n",
    "        x_tim=self.linear_tim(x_tim[:,-1])\n",
    "        return x_act,x_tim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Tax_et_al_mixed(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcat,xcont,yb=dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(4.5081, grad_fn=<AliasBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(p,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log(EventLogs.BPIC_12)\n",
    "traces=split_traces(log)[0][:100]\n",
    "splits=traces[:60],traces[60:80],traces[80:100]\n",
    "splits=split_traces(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _RUN_TRAINING:\n",
    "    datetify=Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed'])\n",
    "    o=PPObj(log,[Categorify,OneHot,datetify,Normalize],cat_names='activity',splits=splits,\n",
    "            date_names='timestamp',y_names=['activity','timestamp_Relative_elapsed'])\n",
    "    m=Tax_et_al_mixed(o)  \n",
    "    dls=o.get_dls()\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    metrics=get_metrics(o)\n",
    "    train_validate(dls,m,loss=loss,metrics=metrics,epoch=1,output_index=[1,2],lr_find=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PPM_Tax_Spezialized(PPModel):\n",
    "\n",
    "    model = Tax_et_al_spezialized\n",
    "    date_names=['timestamp']\n",
    "    cat_names=['activity']\n",
    "    y_names=['activity','timestamp_Relative_elapsed']\n",
    "    cont_names=None\n",
    "    procs=[Categorify,OneHot,Datetify(date_encodings=['secSinceSunNoon','secSinceNoon','Relative_elapsed']),\n",
    "           Normalize,FillMissing]\n",
    "\n",
    "    def setup(self):\n",
    "        o=PPObj(self.log,self.procs,cat_names=self.cat_names,date_names=self.date_names,y_names=self.y_names,\n",
    "                cont_names=self.cont_names,splits=self.splits)\n",
    "\n",
    "        loss=partial(multi_loss_sum,o)\n",
    "\n",
    "        # Next event prediction training\n",
    "        print('Next event prediction training')\n",
    "        dls=o.get_dls(bs=self.bs)\n",
    "        m=self.model(o)\n",
    "        self.nsp,self.dtnp=self._train_validate(dls,m,loss=loss,metrics=get_metrics(o),\n",
    "                                                   output_index=[1,2])\n",
    "\n",
    "\n",
    "        # Last event prediction training\n",
    "        print('Last event prediction training')\n",
    "        dls=o.get_dls(outcome=True,bs=self.bs)\n",
    "        m=self.model(o)\n",
    "        self.op,self.dtlp=self._train_validate(dls,m,loss=loss,metrics=get_metrics(o),\n",
    "                                                 output_index=[1,2])\n",
    "\n",
    "\n",
    "\n",
    "    def next_step_prediction(self): return self.nsp\n",
    "\n",
    "\n",
    "    def outcome_prediction(self): return self.op\n",
    "    def duration_to_next_event_prediction(self): return self.dtnp\n",
    "    def duration_to_end_prediction(self): return self.dtlp\n",
    "    def activity_suffix_prediction(self): pass\n",
    "    def resource_suffix_prediction(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PPM_Tax_Shared(PPM_Tax_Spezialized):\n",
    "    model = Tax_et_al_shared\n",
    "\n",
    "class PPM_Tax_Mixed(PPM_Tax_Spezialized):\n",
    "    model = Tax_et_al_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    log=import_log(path)\n",
    "    ppm=PPM_Tax_Mixed(log,get_ds_name(path),sample=True,print_output=True,epoch=1,bs=512,splits=split_traces(log))\n",
    "    ppm.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: multi categorical atts and multi cont atts  \n",
    "**Output**: activity or resource or duration  \n",
    "**Loss**: cross_entropy(activity) or cross_entropy(resource) or mae(duration) in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MiDA(Module):\n",
    "    def __init__(self,o,seq_len=64) :\n",
    "        super().__init__()\n",
    "        hidden_dim1=100\n",
    "        hidden_dim2=100\n",
    "\n",
    "        out=o.y_names[0]\n",
    "        emb_szs=[(len(o.procs.categorify[c]),len(o.procs.categorify[c])//2 ) for c in o.cat_names ]\n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.n_cont=len(o.cont_names)\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.lstm1=nn.LSTM(self.n_cont+self.n_emb, hidden_dim1, batch_first=True, num_layers=1)\n",
    "\n",
    "        self.bn_cont = nn.BatchNorm1d(self.n_cont)\n",
    "        self.lstm2=nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, num_layers=1)\n",
    "        #self.bn2=nn.BatchNorm1d(seq_len)\n",
    "        #self.bn1=nn.BatchNorm1d(seq_len)\n",
    "\n",
    "        if out in  o.procs.categorify.classes:\n",
    "            self.lin=nn.Linear(hidden_dim2,len(o.procs.categorify[out]))\n",
    "            self.is_classifier=True\n",
    "        else:\n",
    "            self.lin=nn.Linear(hidden_dim2,1)\n",
    "            self.is_classifier=False\n",
    "\n",
    "\n",
    "    def forward(self, x_cat,x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 2)\n",
    "        if self.n_cont != 0:\n",
    "            if self.n_cont == 1: x_cont=x_cont[:,None]\n",
    "            if self.bn_cont: x_cont=self.bn_cont(x_cont).transpose(2,1)\n",
    "            x = torch.cat([x, x_cont], 2) if self.n_emb != 0 else x_cont\n",
    "\n",
    "        x,_=self.lstm1(x)\n",
    "        #x= self.bn1(x)\n",
    "        x,h=self.lstm2(x)\n",
    "        #x=self.bn2(x[:,-1])\n",
    "        x=self.lin(x[:,-1])\n",
    "        if self.is_classifier: x=F.softmax(x,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_log(EventLogs.Helpdesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=PPObj(log,[Categorify,Normalize,Datetify,FillMissing],\n",
    "        cat_names=['activity','resource'],date_names=['timestamp'],y_names='activity',splits=split_traces(log))\n",
    "dls=o.get_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcat,xcont,yb=dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=MiDA(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m(xcat,xcont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if  _RUN_TRAINING:\n",
    "    log=import_log(EventLogs.BPIC_12)\n",
    "    o=PPObj(log,[Categorify,Normalize,Datetify,FillMissing],\n",
    "            cat_names=['activity','resource'],date_names=['timestamp'],\n",
    "            splits=split_traces(log))\n",
    "    o.set_y_names('timestamp_Relative_elapsed')\n",
    "    dls=o.get_dls(bs=512)\n",
    "    seq_len=o.items.event_id.max()\n",
    "    m=MiDA(o,seq_len)\n",
    "    loss=partial(multi_loss_sum,o)\n",
    "    metrics=get_metrics(o)\n",
    "    train_validate(dls,m,epoch=5,loss=loss,metrics=metrics,print_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequences_fast??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class PPM_MiDA(PPModel):\n",
    "    model = MiDA\n",
    "\n",
    "    procs=[Categorify,Normalize,Datetify,FillMissing]\n",
    "\n",
    "    def _attr_from_dict(self,ds_name):\n",
    "        if not self.attr_dict: raise AttributeError('attr_dict is required!')\n",
    "\n",
    "        return (listify(self.attr_dict[self.ds_name]['cat attr']),\n",
    "                listify(self.attr_dict[self.ds_name]['num attr']),\n",
    "                listify(self.attr_dict[self.ds_name]['date attr']))\n",
    "\n",
    "    def setup(self):\n",
    "        cat_names,cont_names,date_names=self._attr_from_dict(self.ds_name)\n",
    "        self.o=PPObj(self.log,[Categorify,Normalize,Datetify,FillMissing],\n",
    "                     cat_names=cat_names,date_names=date_names,cont_names=cont_names,\n",
    "                     splits=self.splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def next_step_prediction(self,col='activity',outcome=False):\n",
    "        seq_len=(self.o.items.event_id.max()) # seq_len = max trace len, Todo make it nicer\n",
    "        self.o.set_y_names(col)\n",
    "        print(self.o.y_names)\n",
    "        dls=self.o.get_dls(bs=self.bs,outcome=outcome)\n",
    "        m=self.model(self.o,seq_len)\n",
    "        loss=partial(multi_loss_sum,self.o)\n",
    "        metrics=get_metrics(self.o)\n",
    "        return self._train_validate(dls,m,loss=loss,metrics=metrics)\n",
    "\n",
    "    def next_resource_prediction(self):return self.next_step_prediction(outcome=False,col='resource')\n",
    "\n",
    "    def last_resource_prediction(self): return self.next_step_prediction(outcome=True,col='resource')\n",
    "    def outcome_prediction(self): return self.next_step_prediction(outcome=True,col='activity')\n",
    "\n",
    "    def duration_to_next_event_prediction(self):\n",
    "        return self.next_step_prediction(outcome=False,col='timestamp_Relative_elapsed')\n",
    "\n",
    "    def duration_to_end_prediction(self):\n",
    "        return self.next_step_prediction(outcome=True,col='timestamp_Relative_elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_attr_dict(attr_list):\n",
    "    attr_df=pd.DataFrame(attr_list,columns=['name','cat attr','num attr','date attr'])\n",
    "    attr_df.index=attr_df.name\n",
    "    attr_df.drop('name',axis=1,inplace=True)\n",
    "    attr_df.index.name=\"\"\n",
    "    attr_dict=attr_df.apply(lambda x:(x.str.split(', '))).T.to_dict()\n",
    "    return attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "attr_list=[\n",
    "    ['BPIC12','activity, resource','AMOUNT_REQ','timestamp'],\n",
    "    ['BPIC12_W','activity, resource','AMOUNT_REQ','timestamp'],\n",
    "    ['BPIC12_Wc','activity, resource','AMOUNT_REQ','timestamp'],\n",
    "    ['BPIC12_O','activity, resource','AMOUNT_REQ','timestamp'],\n",
    "    ['BPIC12_A','activity, resource','AMOUNT_REQ','timestamp'],\n",
    "    ['Mobis','activity, resource, type','cost','timestamp'],\n",
    "    ['BPIC13_CP','activity, resource, resource country, organization country, organization involved, impact, product, org:role',\n",
    "     None,'timestamp'],\n",
    "    ['Helpdesk','activity, resource',None,'timestamp'],\n",
    "    ['BPIC17_O','activity, Action, NumberOfTerms, resource',\n",
    "     'FirstWithdrawalAmount, MonthlyCost, OfferedAmount, CreditScore', 'timestamp'],\n",
    "    ['BPIC20_RFP','org:role, activity, resource, Project, Task, OrganizationalEntity',\n",
    "     'RequestedAmount','timestamp']\n",
    "]\n",
    "\n",
    "\n",
    "attr_dict=create_attr_dict(attr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if _RUN_TRAINING:\n",
    "    path=EventLogs.Helpdesk\n",
    "    log=import_log(path)\n",
    "    ppm=PPM_MiDA(log,get_ds_name(path),print_output=True,epoch=1,bs=512,attr_dict=attr_dict,splits=split_traces(log))\n",
    "    ppm.setup()\n",
    "    ppm.next_step_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mppn",
   "language": "python",
   "name": "mppn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
